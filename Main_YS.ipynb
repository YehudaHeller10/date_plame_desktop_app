{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from numpy import mean, array\n",
    "from numpy import std\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "# import arrow\n",
    "import pwlf\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "import seaborn as sns\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib as mlp\n",
    "from scipy import stats\n",
    "import scipy\n",
    "\n",
    "from math import log\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "# import chardet\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs, SequentialFeatureSelector\n",
    "from matplotlib import pyplot as plt, pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import RepeatedKFold, RepeatedStratifiedKFold, StratifiedKFold, KFold, cross_val_predict\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from xgboost import XGBRegressor\n",
    "# import xgboost as xgb\n",
    "# import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_code = os.path.dirname(os.path.realpath(\"file\"))\n",
    "path_files = path_code.replace('\\\\code\\\\Data processing', '\\\\data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Files (some are processed?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counting_2022_2023 = pd.read_excel(path_files + \"\\\\Raw data\\\\counting_2022_2023_MB-20240818.xlsx\", sheet_name=\"לתמריקה (3)\")\n",
    "counting_2022_2023.dropna(axis = 0, how = 'all', inplace = True)\n",
    "counting_2022_2023.rename(columns={ 'HAVA:PLOT': 'file_name'}, inplace=True)\n",
    "\n",
    "\n",
    "processed_data_2012_2021 = pd.read_excel(path_files + \"\\\\Processed data\\\\processed_data_2012_2020_final_hopefully.xlsx\")\n",
    "processed_counting_2022_2023 = pd.read_excel(path_files + \"\\\\Processed data\\\\counting_2022_2023_processed.xlsx\")\n",
    "processed_data = pd.concat([processed_data_2012_2021, processed_counting_2022_2023])\n",
    "# processed_data = pd.read_excel(path_files + \"\\\\Processed data\\\\processed_data_2012_2023.xlsx\")\n",
    "# homogemic_df = pd.read_excel(path_files + \"\\\\Processed data\\\\Homogenic plots 2012-2023.xlsx\")\n",
    "# heterogenic_df = pd.read_excel(path_files + \"\\\\Processed data\\\\Heterogenic plots 2012-2023.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_data_quality = quality_processing(processed_data)\n",
    "processed_data_quality.to_excel(path_files + \"\\\\Processed data\\\\YS_processed_data_2012_2023.xlsx\")\n",
    "# processed_data_quality = quality_processing(heterogenic_df)\n",
    "# processed_data_quality.to_excel(path_files + \"\\\\Processed data\\\\Heterogenic plots 2012-2023.xlsx\")\n",
    "# trees_age = pd.read_excel(path_files + \"\\\\Processed data\\\\Trees per plot.xlsx\")\n",
    "# trees_age = trees_age[[\"HAVA:PLOT\", \"שנת נטיעה\"]]\n",
    "# processed_data = pd.merge(processed_data,trees_age, left_on=\"Farm:Plot\", right_on=\"HAVA:PLOT\")\n",
    "# processed_data = processed_data.drop('HAVA:PLOT', axis=1)\n",
    "# processed_data = quality_processing(processed_data)\n",
    "# processed_data.rename(columns={'שנת נטיעה': 'Planting year'}, inplace=True)\n",
    "# processed_data['Age'] = processed_data['Year'] - processed_data['Planting year']\n",
    "# processed_data.to_excel(path_files + \"\\\\Processed data\\\\processed_data_2012_2023.xlsx\")\n",
    "# df_results = basic_model(processed_data, feature_dict, quality_labels)\n",
    "# df_results.to_excel(path_files + \"\\\\Model results\\\\All plots (without age as feature)\\\\Quality model results 2012-2023 without age_2.xlsx\")\n",
    "\n",
    "# df_results = basic_model(homogemic_df, feature_dict_with_age, quality_labels)\n",
    "# df_results.to_excel(path_files + \"\\\\Model results\\\\Homogenic plots (with age as feature)\\\\Quality model results 2012-2023 with age.xlsx\")\n",
    "# all_features_without_monthly_with_age = all_features_without_monthly.copy()\n",
    "# all_features_without_monthly_with_age.append(\"Age\")\n",
    "# counts_with_age = counts.copy()\n",
    "# counts_with_age.append(\"Age\")\n",
    "# Feature_analysis(processed_data, counts, \"Reslative_yield_per_tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: initial_processing_counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_processing_counting(original_df):\n",
    "    processed_df = pd.DataFrame()\n",
    "\n",
    "    for farm_plot in original_df['file_name'].unique():\n",
    "        for date in original_df.loc[original_df['file_name'] == farm_plot]['DAY'].unique():\n",
    "\n",
    "            temp_df = original_df.loc[(original_df['file_name'] == farm_plot) & (original_df['DAY'] == date)]\n",
    "\n",
    "            # Get days since January\n",
    "            year = date[0:4]\n",
    "            a = arrow.get(f'{year}-01-01') #january 1st of that year\n",
    "            b = arrow.get(date[0:10]) # the date\n",
    "            delta = (b - a)\n",
    "\n",
    "            AVG_fruitlets_upper = float(\"NaN\")\n",
    "            AVG_fruitlets_center = float(\"NaN\")\n",
    "            AVG_fruitlets_lower = float(\"NaN\")\n",
    "\n",
    "            num_samples_upper = 0\n",
    "            num_samples_center = 0\n",
    "            num_samples_lower = 0\n",
    "\n",
    "            # UPPER\n",
    "            # If bunch A and bunch B both aren't empty (if not all values are nan in both of them):\n",
    "            if not (temp_df['BUNCHA'].isnull().values.all()) | (temp_df['BUNCHB'].isnull().values.all()):\n",
    "                num_samples_upper = 2\n",
    "                AVG_fruitlets_upper = ((temp_df['QA1'].mean() + temp_df['QA2'].mean() + temp_df['QA3'].mean() + temp_df['QA4'].mean() +\n",
    "                                        temp_df['QB1'].mean() + temp_df['QB2'].mean() + temp_df['QB3'].mean() +\n",
    "                                        temp_df['QB4'].mean()) / 8) * ((temp_df['BUNCHA'].mean() + temp_df['BUNCHB'].mean()) / 2)\n",
    "            # If bunch A isn't empty but bunch B is:\n",
    "            elif not (temp_df['BUNCHA'].isnull().values.all()):\n",
    "                num_samples_upper = 1\n",
    "                AVG_fruitlets_upper = ((temp_df['QA1'].mean() + temp_df['QA2'].mean() + temp_df['QA3'].mean() + temp_df['QA4'].mean())\n",
    "                                       / 4) * temp_df['BUNCHA'].mean()\n",
    "\n",
    "            # If bunch B isn't empty but bunch A is:\n",
    "            elif not (temp_df['BUNCHB'].isnull().values.all()):\n",
    "                num_samples_upper = 1\n",
    "                AVG_fruitlets_upper = ((temp_df['QB1'].mean() + temp_df['QB2'].mean() + temp_df['QB3'].mean() + temp_df['QB4'].mean())\n",
    "                                       / 4) * temp_df['BUNCHB'].mean()\n",
    "\n",
    "\n",
    "            # CENTER\n",
    "\n",
    "            if not (temp_df['BUNCHC'].isnull().values.all()) | (temp_df['BUNCHD'].isnull().values.all()):\n",
    "                num_samples_center = 2\n",
    "                AVG_fruitlets_center = ((temp_df['QC1'].mean() + temp_df['QC2'].mean() + temp_df['QC3'].mean() + temp_df['QC4'].mean() +\n",
    "                                        temp_df['QD1'].mean() + temp_df['QD2'].mean() + temp_df['QD3'].mean() +\n",
    "                                        temp_df['QD4'].mean()) / 8) * ( (temp_df['BUNCHC'].mean() + temp_df['BUNCHD'].mean()) / 2)\n",
    "\n",
    "            elif not (temp_df['BUNCHC'].isnull().values.all()):\n",
    "                num_samples_center = 1\n",
    "                AVG_fruitlets_center = ((temp_df['QC1'].mean() + temp_df['QC2'].mean() + temp_df['QC3'].mean() +\n",
    "                                        temp_df['QC4'].mean()) / 4) * temp_df['BUNCHC'].mean()\n",
    "\n",
    "            elif not (temp_df['BUNCHD'].isnull().values.all()):\n",
    "                num_samples_center = 1\n",
    "                AVG_fruitlets_center = ((temp_df['QD1'].mean() + temp_df['QD2'].mean() + temp_df['QD3'].mean() +\n",
    "                                        temp_df['QD4'].mean()) / 4) * temp_df['BUNCHD'].mean()\n",
    "\n",
    "            # LOWER\n",
    "\n",
    "            if not (temp_df['BUNCHE'].isnull().values.all()) | (temp_df['BUNCHF'].isnull().values.all()):\n",
    "                num_samples_lower = 2\n",
    "                AVG_fruitlets_lower = ((temp_df['QE1'].mean() + temp_df['QE2'].mean() + temp_df['QE3'].mean() + temp_df['QE4'].mean() +\n",
    "                                        temp_df['QF1'].mean() + temp_df['QF2'].mean() + temp_df['QF3'].mean() +\n",
    "                                        temp_df['QF4'].mean()) / 8) * ( (temp_df['BUNCHE'].mean() + temp_df['BUNCHF'].mean()) / 2)\n",
    "            elif not (temp_df['BUNCHE'].isnull().values.all()):\n",
    "                num_samples_lower = 1\n",
    "                AVG_fruitlets_lower = ((temp_df['QE1'].mean() + temp_df['QE2'].mean() + temp_df['QE3'].mean() +\n",
    "                                        temp_df['QE4'].mean()) / 4) * temp_df['BUNCHE'].mean()\n",
    "            elif not (temp_df['BUNCHF'].isnull().values.all()):\n",
    "                num_samples_lower = 1\n",
    "                AVG_fruitlets_lower = ((temp_df['QF1'].mean() + temp_df['QF2'].mean() + temp_df['QF3'].mean() +\n",
    "                                        temp_df['QF4'].mean()) / 4) * temp_df['BUNCHF'].mean()\n",
    "\n",
    "            # If we have all three (upper, center, lower):\n",
    "\n",
    "            if not (math.isnan(AVG_fruitlets_upper)) | (math.isnan(AVG_fruitlets_upper)) | (math.isnan(AVG_fruitlets_upper)):\n",
    "                SUM_tree_fruitlets =  temp_df['TBRANCH'].mean() * (1/4 * AVG_fruitlets_upper + 1/2 * AVG_fruitlets_center + 1/4 * AVG_fruitlets_lower)\n",
    "\n",
    "            else:\n",
    "                SUM_tree_fruitlets = float(\"NaN\")\n",
    "\n",
    "\n",
    "            processed_df = processed_df.append({'Farm:Plot': farm_plot, 'Year' :year, 'Date':date[0:10], 'Days since january 1st': delta.days,\n",
    "                                                'AVG_clusters': temp_df['TBRANCH'].mean(), 'AVG_fruitlets_upper': AVG_fruitlets_upper,\n",
    "                                                'NUM_samples_upper' : num_samples_upper, 'AVG_fruitlets_center': AVG_fruitlets_center,\n",
    "                                                'NUM_samples_center' : num_samples_center,'AVG_fruitlets_lower': AVG_fruitlets_lower,\n",
    "                                                'NUM_samples_lower' : num_samples_lower,'SUM_tree_fruitlets':SUM_tree_fruitlets},\n",
    "                                               ignore_index=True)\n",
    "    return processed_df\n",
    "\n",
    "def initial_processing_counting_6_samples(original_df):\n",
    "    processed_df = pd.DataFrame()\n",
    "\n",
    "    original_df['DAY'] = original_df['DAY'].astype(str)\n",
    "\n",
    "    for farm_plot in original_df['file_name'].unique():\n",
    "        for date in original_df.loc[original_df['file_name'] == farm_plot]['DAY'].unique():\n",
    "\n",
    "            temp_df = original_df.loc[(original_df['file_name'] == farm_plot) & (original_df['DAY'] == date)]\n",
    "\n",
    "            # Get days since January\n",
    "            year = date[0:4]\n",
    "            month = date[4:6]\n",
    "            day = date[6:8]\n",
    "            a = arrow.get(f'{year}-01-01') #january 1st of that year\n",
    "            b = arrow.get(f'{year}-{month}-{day}') # the date\n",
    "            delta = (b - a)\n",
    "            date_string = f'{year}-{month}-{day}'\n",
    "\n",
    "            AVG_fruitlets_upper = float(\"NaN\")\n",
    "            AVG_fruitlets_center = float(\"NaN\")\n",
    "            AVG_fruitlets_lower = float(\"NaN\")\n",
    "\n",
    "            num_samples_upper = 0\n",
    "            num_samples_center = 0\n",
    "            num_samples_lower = 0\n",
    "\n",
    "            # UPPER\n",
    "            # If bunch A and bunch B both aren't empty (if not all values are nan in both of them):\n",
    "            if not (temp_df['BUNCHA'].isnull().values.all()) | (temp_df['BUNCHB'].isnull().values.all()):\n",
    "                num_samples_upper = 2\n",
    "                AVG_fruitlets_upper = ((temp_df['QA1'].mean() + temp_df['QA2'].mean() + temp_df['QA3'].mean() + temp_df['QA4'].mean() +\n",
    "                                        temp_df['QA5'].mean() + temp_df['QA6'].mean() +\n",
    "                                        temp_df['QB1'].mean() + temp_df['QB2'].mean() + temp_df['QB3'].mean() +\n",
    "                                        temp_df['QB4'].mean()+ temp_df['QB5'].mean()+\n",
    "                                        temp_df['QB6'].mean() ) / 12) * ((temp_df['BUNCHA'].mean() + temp_df['BUNCHB'].mean()) / 2)\n",
    "            # If bunch A isn't empty but bunch B is:\n",
    "            elif not (temp_df['BUNCHA'].isnull().values.all()):\n",
    "                num_samples_upper = 1\n",
    "                AVG_fruitlets_upper = ((temp_df['QA1'].mean() + temp_df['QA2'].mean() + temp_df['QA3'].mean() + temp_df['QA4'].mean()\n",
    "                                        + temp_df['QA5'].mean() + temp_df['QA6'].mean())\n",
    "                                       / 6) * temp_df['BUNCHA'].mean()\n",
    "\n",
    "            # If bunch B isn't empty but bunch A is:\n",
    "            elif not (temp_df['BUNCHB'].isnull().values.all()):\n",
    "                num_samples_upper = 1\n",
    "                AVG_fruitlets_upper = ((temp_df['QB1'].mean() + temp_df['QB2'].mean() + temp_df['QB3'].mean() + temp_df['QB4'].mean()\n",
    "                                        + temp_df['QB5'].mean() + temp_df['QB6'].mean())\n",
    "                                       / 6) * temp_df['BUNCHB'].mean()\n",
    "\n",
    "\n",
    "            # CENTER\n",
    "\n",
    "            if not (temp_df['BUNCHC'].isnull().values.all()) | (temp_df['BUNCHD'].isnull().values.all()):\n",
    "                num_samples_center = 2\n",
    "                AVG_fruitlets_center = ((temp_df['QC1'].mean() + temp_df['QC2'].mean() + temp_df['QC3'].mean() + temp_df['QC4'].mean() +\n",
    "                                         temp_df['QC5'].mean() + temp_df['QC6'].mean() +\n",
    "                                        temp_df['QD1'].mean() + temp_df['QD2'].mean() + temp_df['QD3'].mean() + temp_df['QD4'].mean() +\n",
    "                                         temp_df['QD5'].mean() +\n",
    "                                        temp_df['QD6'].mean() ) / 12) * ( (temp_df['BUNCHC'].mean() + temp_df['BUNCHD'].mean()) / 2)\n",
    "\n",
    "            elif not (temp_df['BUNCHC'].isnull().values.all()):\n",
    "                num_samples_center = 1\n",
    "                AVG_fruitlets_center = ((temp_df['QC1'].mean() + temp_df['QC2'].mean() + temp_df['QC3'].mean() +\n",
    "                                         temp_df['QC4'].mean() + temp_df['QC5'].mean() +\n",
    "                                        temp_df['QC6'].mean()) / 6) * temp_df['BUNCHC'].mean()\n",
    "\n",
    "            elif not (temp_df['BUNCHD'].isnull().values.all()):\n",
    "                num_samples_center = 1\n",
    "                AVG_fruitlets_center = ((temp_df['QD1'].mean() + temp_df['QD2'].mean() + temp_df['QD3'].mean() +\n",
    "                                         temp_df['QD4'].mean() + temp_df['QD5'].mean() +\n",
    "                                        temp_df['QD6'].mean()) / 6) * temp_df['BUNCHD'].mean()\n",
    "\n",
    "            # LOWER\n",
    "\n",
    "            if not (temp_df['BUNCHE'].isnull().values.all()) | (temp_df['BUNCHF'].isnull().values.all()):\n",
    "                num_samples_lower = 2\n",
    "                AVG_fruitlets_lower = ((temp_df['QE1'].mean() + temp_df['QE2'].mean() + temp_df['QE3'].mean() + temp_df['QE4'].mean() +\n",
    "                                        temp_df['QE5'].mean() + temp_df['QE6'].mean() +\n",
    "                                        temp_df['QF1'].mean() + temp_df['QF2'].mean() + temp_df['QF3'].mean() +\n",
    "                                        temp_df['QF4'].mean() + temp_df['QF5'].mean() +\n",
    "                                        temp_df['QF6'].mean()) / 12) * ( (temp_df['BUNCHE'].mean() + temp_df['BUNCHF'].mean()) / 2)\n",
    "            elif not (temp_df['BUNCHE'].isnull().values.all()):\n",
    "                num_samples_lower = 1\n",
    "                AVG_fruitlets_lower = ((temp_df['QE1'].mean() + temp_df['QE2'].mean() + temp_df['QE3'].mean() +\n",
    "                                        temp_df['QE4'].mean() + temp_df['QE5'].mean() +\n",
    "                                        temp_df['QE6'].mean()) / 6) * temp_df['BUNCHE'].mean()\n",
    "            elif not (temp_df['BUNCHF'].isnull().values.all()):\n",
    "                num_samples_lower = 1\n",
    "                AVG_fruitlets_lower = ((temp_df['QF1'].mean() + temp_df['QF2'].mean() + temp_df['QF3'].mean() +\n",
    "                                        temp_df['QF4'].mean() + temp_df['QF5'].mean() +\n",
    "                                        temp_df['QF6'].mean()) / 6) * temp_df['BUNCHF'].mean()\n",
    "\n",
    "            # If we have all three (upper, center, lower):\n",
    "\n",
    "            if not (math.isnan(AVG_fruitlets_upper)) | (math.isnan(AVG_fruitlets_upper)) | (math.isnan(AVG_fruitlets_upper)):\n",
    "                SUM_tree_fruitlets =  temp_df['TBRANCH'].mean() * (1/4 * AVG_fruitlets_upper + 1/2 * AVG_fruitlets_center + 1/4 * AVG_fruitlets_lower)\n",
    "\n",
    "            else:\n",
    "                SUM_tree_fruitlets = float(\"NaN\")\n",
    "\n",
    "\n",
    "            processed_df = processed_df.append({'Farm:Plot': farm_plot, 'Year' :year, 'Date':date_string, 'Days since january 1st': delta.days,\n",
    "                                                'AVG_clusters': temp_df['TBRANCH'].mean(), 'AVG_fruitlets_upper': AVG_fruitlets_upper,\n",
    "                                                'NUM_samples_upper' : num_samples_upper, 'AVG_fruitlets_center': AVG_fruitlets_center,\n",
    "                                                'NUM_samples_center' : num_samples_center,'AVG_fruitlets_lower': AVG_fruitlets_lower,\n",
    "                                                'NUM_samples_lower' : num_samples_lower,'SUM_tree_fruitlets':SUM_tree_fruitlets},\n",
    "                                               ignore_index=True)\n",
    "    return processed_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: weather_data, weather_data_Flowering_periods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_data(): #processed_df\n",
    "    print('a')\n",
    "    # df = pd.read_excel(path_files + \"\\\\Processed data\\\\Combined Meteorological data.xlsx\")\n",
    "    print('b')\n",
    "\n",
    "    df_2011 = pd.read_excel(path_files + '\\\\Raw data/Meteorology Yotvata 2011 radiation.xlsx')\n",
    "    df_2012 = pd.read_excel(path_files + '\\\\Raw data/Meteorology Yotvata 2012 radiation.xlsx')\n",
    "    df_2013 = pd.read_excel(path_files + '\\\\Raw data/Meteorology Yotvata 2013 radiation.xlsx')\n",
    "    df_2014 = pd.read_excel(path_files + '\\\\Raw data/Meteorology Yotvata 2014 radiation.xlsx')\n",
    "    df_2015 = pd.read_excel(path_files + '\\\\Raw data/Meteorology Yotvata 2015 radiation.xlsx')\n",
    "    df_2016 = pd.read_excel(path_files + '\\\\Raw data/Meteorology Yotvata 2016 radiation.xlsx')\n",
    "    df_2017 = pd.read_excel(path_files + '\\\\Raw data/Meteorology Yotvata 2017 radiation.xlsx')\n",
    "    df_2018 = pd.read_excel(path_files + '\\\\Raw data/Meteorology Yotvata 2018 radiation.xlsx')\n",
    "    df_2019 = pd.read_excel(path_files + '\\\\Raw data/Meteorology Yotvata 2019 radiation.xlsx')\n",
    "    df_2020 = pd.read_excel(path_files + '\\\\Raw data/Meteorology Yotvata 2020 radiation.xlsx')\n",
    "    df_2021 = pd.read_excel(path_files + '\\\\Raw data/Meteorology Yotvata 2021 radiation.xlsx')\n",
    "    df_2022 = pd.read_excel(path_files + '\\\\Raw data/Meteorology Yotvata 2022 radiation.xlsx')\n",
    "    df_2023 = pd.read_excel(path_files + '\\\\Raw data/Meteorology Yotvata 2023 radiation.xlsx')\n",
    "    df_list = [df_2011, df_2012, df_2013, df_2014, df_2015, df_2016, df_2017, df_2018, df_2019, df_2020, df_2021, df_2022, df_2023]\n",
    "    # df_list = [df_2013]\n",
    "    df = pd.concat(df_list)\n",
    "    #df.to_excel(path_files + \"\\\\Processed data\\\\Combined Meteorological data 2012 2023.xlsx\")\n",
    "    df = df.drop_duplicates()\n",
    "    df.rename(columns={'תחנה': 'Station',\n",
    "                      'תאריך ושעה (שעון חורף)': 'Date & Time (Winter)',\n",
    "                      'לחות יחסית (%)': 'Relative humidity (%)',\n",
    "                      'טמפרטורה (C°)': 'Temperature (°C)',\n",
    "                      'כמות גשם (מ\"מ)': 'Rainfall (mm)',\n",
    "                      'קרינה ישירה (וואט/מ\"ר)': 'Radiation (W/m2)'}, inplace=True)\n",
    "\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date & Time (Winter)'], dayfirst=True).dt.date\n",
    "    df['Time'] = pd.to_datetime(df['Date & Time (Winter)'], dayfirst=True).dt.time\n",
    "    df['hour'] = pd.to_datetime(df['Date & Time (Winter)'], dayfirst=True).dt.hour\n",
    "    df['month'] = pd.to_datetime(df['Date & Time (Winter)'], dayfirst=True).dt.month\n",
    "    df['year'] = pd.to_datetime(df['Date & Time (Winter)'], dayfirst=True).dt.year\n",
    "\n",
    "    df['Temperature (°C)'] = pd.to_numeric(df['Temperature (°C)'], errors='coerce')\n",
    "    df['Relative humidity (%)'] = pd.to_numeric(df['Relative humidity (%)'], errors='coerce')\n",
    "    df.to_excel(path_files + \"\\\\Processed data\\\\Combined Meteorological data 2011 2023.xlsx\")\n",
    "\n",
    "    heat_thresholod = 18\n",
    "\n",
    "    conditions = [\n",
    "        (df['Temperature (°C)'] > heat_thresholod),\n",
    "        (df['Temperature (°C)'] <= heat_thresholod)\n",
    "    ]\n",
    "\n",
    "    values = [(df['Temperature (°C)'] - heat_thresholod) * 0.16666667, 0]\n",
    "\n",
    "    df['degree_hours_10_min'] = np.select(conditions, values)\n",
    "\n",
    "    df_temperature = df.groupby(['Station', 'year', 'month']).degree_hours_10_min.sum().reset_index()\n",
    "    df_temperature = df_temperature.pivot(index=['Station', 'year'], columns='month', values='degree_hours_10_min')\n",
    "    df_temperature.columns = ['t1', 't2', 't3', 't4', 't5', 't6', 't7', 't8', 't9', 't10', 't11', 't12']\n",
    "    df_temperature['t10'] = df_temperature['t10'].shift(periods=1)\n",
    "    df_temperature['t11'] = df_temperature['t11'].shift(periods=1)\n",
    "    df_temperature['t12'] = df_temperature['t12'].shift(periods=1)\n",
    "\n",
    "    df_humidity = df.groupby(['Station', 'year', 'month'])['Relative humidity (%)'].mean().reset_index()\n",
    "    df_humidity = df_humidity.pivot(index=['Station', 'year'], columns='month', values='Relative humidity (%)')\n",
    "    df_humidity.columns = ['h1',\n",
    "                           'h2',\n",
    "                           'h3',\n",
    "                           'h4',\n",
    "                           'h5',\n",
    "                           'h6',\n",
    "                           'h7',\n",
    "                           'h8',\n",
    "                           'h9',\n",
    "                           'h10',\n",
    "                           'h11',\n",
    "                           'h12']\n",
    "\n",
    "    df_humidity['h10'] = df_humidity['h10'].shift(periods=1)\n",
    "    df_humidity['h11'] = df_humidity['h11'].shift(periods=1)\n",
    "    df_humidity['h12'] = df_humidity['h12'].shift(periods=1)\n",
    "\n",
    "    df_meteorology = pd.merge(df_temperature, df_humidity, on='year', how='outer')\n",
    "\n",
    "    #final_df = pd.merge(processed_df, df_meteorology, left_on='Year', right_on='year', how='outer')\n",
    "\n",
    "    return df_meteorology #final_df\n",
    "\n",
    "def weather_data_Flowering_periods(meteorological_data): #processed_df_weather\n",
    "\n",
    "    df_periods = pd.DataFrame()\n",
    "\n",
    "    # processed_df_weather = processed_df_weather[processed_df_weather['Year'].notna()]\n",
    "    # processed_df_weather['Year'] = processed_df_weather['Year'].astype('int64')\n",
    "\n",
    "    heat_thresholod = 18\n",
    "\n",
    "    conditions = [\n",
    "        (meteorological_data['Temperature (°C)'] > heat_thresholod),\n",
    "        (meteorological_data['Temperature (°C)'] <= heat_thresholod)\n",
    "    ]\n",
    "\n",
    "    values = [(meteorological_data['Temperature (°C)'] - heat_thresholod) * 0.16666667, 0]\n",
    "\n",
    "    meteorological_data['degree_hours_10_min'] = np.select(conditions, values)\n",
    "\n",
    "    for i in meteorological_data['year'].unique():\n",
    "        df_periods = pd.concat([df_periods, pd.DataFrame([{'year': i, \"T_Inf_differentiation\": meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i-1}-11-01',\n",
    "                                                                               format='%Y-%m-%d')) & (\n",
    "                                            meteorological_data['Date'] <= pd.to_datetime(f'{i}-02-10',\n",
    "                                                                                          format='%Y-%m-%d'))]['degree_hours_10_min'].sum()\n",
    "                              , \"T_Flowering\":\n",
    "                               meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i}-02-11',format='%Y-%m-%d')) & (\n",
    "                                                               meteorological_data['Date'] <= pd.to_datetime( f'{i}-03-31',format='%Y-%m-%d'))]\n",
    "                               ['degree_hours_10_min'].sum()\n",
    "\n",
    "                              , \"T_Thinning\":\n",
    "                               meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i}-04-01',format='%Y-%m-%d')) & (\n",
    "                                                               meteorological_data['Date'] <= pd.to_datetime(f'{i}-05-15',format='%Y-%m-%d'))]\n",
    "                               ['degree_hours_10_min'].sum()\n",
    "\n",
    "                              , \"T_Growth\":\n",
    "                               meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i}-05-16',format='%Y-%m-%d')) & (\n",
    "                                                               meteorological_data['Date'] <= pd.to_datetime(f'{i}-07-31',\n",
    "                                                           format='%Y-%m-%d'))]\n",
    "                               ['degree_hours_10_min'].sum()\n",
    "\n",
    "                              , \"T_June_Drop\":\n",
    "                               meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i}-06-01',format='%Y-%m-%d')) & (\n",
    "                                                               meteorological_data['Date'] <= pd.to_datetime(f'{i}-06-30',\n",
    "                                                           format='%Y-%m-%d'))]\n",
    "                               ['degree_hours_10_min'].sum()\n",
    "\n",
    "                              , \"T_Ripening\":\n",
    "                               meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i}-08-01',format='%Y-%m-%d')) & (\n",
    "                                                               meteorological_data['Date'] <= pd.to_datetime(f'{i}-08-31',format='%Y-%m-%d'))]\n",
    "                               ['degree_hours_10_min'].sum()\n",
    "\n",
    "                              , \"T_Harvest\":\n",
    "                               meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i}-09-01',format='%Y-%m-%d')) & (\n",
    "                                                               meteorological_data['Date'] <= pd.to_datetime(f'{i}-10-31',format='%Y-%m-%d'))]\n",
    "                               ['degree_hours_10_min'].sum()\n",
    "\n",
    "                               , \"H_Inf_differentiation\":\n",
    "                                meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i - 1}-11-01',format='%Y-%m-%d')) & (\n",
    "                                                        meteorological_data['Date'] <= pd.to_datetime(f'{i}-02-10',format='%Y-%m-%d'))]\n",
    "                                ['Relative humidity (%)'].mean()\n",
    "\n",
    "                                , \"H_Flowering\":\n",
    "                                meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i}-02-11', format='%Y-%m-%d')) & (\n",
    "                                                                meteorological_data['Date'] <= pd.to_datetime(f'{i}-03-31', format='%Y-%m-%d'))]\n",
    "                                ['Relative humidity (%)'].mean()\n",
    "\n",
    "                                , \"H_Thinning\":\n",
    "                                meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i}-04-01', format='%Y-%m-%d')) & (\n",
    "                                                                meteorological_data['Date'] <= pd.to_datetime(f'{i}-05-15',format='%Y-%m-%d'))]\n",
    "                                ['Relative humidity (%)'].mean()\n",
    "\n",
    "                                , \"H_Growth\":\n",
    "                                meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i}-05-16', format='%Y-%m-%d')) & (\n",
    "                                                                            meteorological_data['Date'] <= pd.to_datetime(f'{i}-07-31', format='%Y-%m-%d'))]\n",
    "                                ['Relative humidity (%)'].mean()\n",
    "\n",
    "                                , \"H_June_Drop\":\n",
    "                                meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i}-06-01', format='%Y-%m-%d')) & (\n",
    "                                                                            meteorological_data['Date'] <= pd.to_datetime(f'{i}-06-30',format='%Y-%m-%d'))]\n",
    "                                ['Relative humidity (%)'].mean()\n",
    "\n",
    "                                , \"H_Ripening\":\n",
    "                                meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i}-08-01', format='%Y-%m-%d')) & (\n",
    "                                                                            meteorological_data['Date'] <= pd.to_datetime(f'{i}-08-31',format='%Y-%m-%d'))]\n",
    "                                ['Relative humidity (%)'].mean()\n",
    "\n",
    "                                , \"H_Harvest\":\n",
    "                                meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i}-09-01', format='%Y-%m-%d')) & (\n",
    "                                                                            meteorological_data['Date'] <= pd.to_datetime(f'{i}-10-31',format='%Y-%m-%d'))]\n",
    "                                ['Relative humidity (%)'].mean()\n",
    "                                \n",
    "                              , \"E_Inf_differentiation\": meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i-1}-11-01',\n",
    "                                                                               format='%Y-%m-%d')) & (\n",
    "                                            meteorological_data['Date'] <= pd.to_datetime(f'{i}-02-10',\n",
    "                                                                                          format='%Y-%m-%d'))]['Evaporation (mm/10min)'].sum()\n",
    "                              , \"E_Flowering\":\n",
    "                               meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i}-02-11',format='%Y-%m-%d')) & (\n",
    "                                                               meteorological_data['Date'] <= pd.to_datetime( f'{i}-03-31',format='%Y-%m-%d'))]\n",
    "                               ['Evaporation (mm/10min)'].sum()\n",
    "\n",
    "                              , \"E_Thinning\":\n",
    "                               meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i}-04-01',format='%Y-%m-%d')) & (\n",
    "                                                               meteorological_data['Date'] <= pd.to_datetime(f'{i}-05-15',format='%Y-%m-%d'))]\n",
    "                               ['Evaporation (mm/10min)'].sum()\n",
    "\n",
    "                              , \"E_Growth\":\n",
    "                               meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i}-05-16',format='%Y-%m-%d')) & (\n",
    "                                                               meteorological_data['Date'] <= pd.to_datetime(f'{i}-07-31',\n",
    "                                                           format='%Y-%m-%d'))]\n",
    "                               ['Evaporation (mm/10min)'].sum()\n",
    "\n",
    "                              , \"E_June_Drop\":\n",
    "                               meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i}-06-01',format='%Y-%m-%d')) & (\n",
    "                                                               meteorological_data['Date'] <= pd.to_datetime(f'{i}-06-30',\n",
    "                                                           format='%Y-%m-%d'))]\n",
    "                               ['Evaporation (mm/10min)'].sum()\n",
    "\n",
    "                              , \"E_Ripening\":\n",
    "                               meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i}-08-01',format='%Y-%m-%d')) & (\n",
    "                                                               meteorological_data['Date'] <= pd.to_datetime(f'{i}-08-31',format='%Y-%m-%d'))]\n",
    "                               ['Evaporation (mm/10min)'].sum()\n",
    "\n",
    "                              , \"E_Harvest\":\n",
    "                               meteorological_data.loc[(meteorological_data['Date'] >= pd.to_datetime(f'{i}-09-01',format='%Y-%m-%d')) & (\n",
    "                                                               meteorological_data['Date'] <= pd.to_datetime(f'{i}-10-31',format='%Y-%m-%d'))]\n",
    "                               ['Evaporation (mm/10min)'].sum()  }])], ignore_index=True)\n",
    "\n",
    "\n",
    "        print('stop')\n",
    "\n",
    "    #  df = pd.merge(processed_df_weather, df_periods, on=\"Year\")\n",
    "    return df_periods #df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "df_meteorology = weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteorology.to_excel(path_files + \"\\\\Processed data\\\\Combined Meteorological data 2011 2023 radiation.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteorological_data = pd.read_excel(path_files + \"\\\\Processed data\\\\Combined Meteorological data 2011 2023.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\salzer\\AppData\\Local\\Temp\\ipykernel_15016\\854858161.py:51: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  meteorological_data[columns_to_check] = meteorological_data[columns_to_check].applymap(lambda x: np.nan if x < 0 else x)\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "rho = 1.2  # Air density (kg/m³)\n",
    "cp = 1013  # Specific heat of air (J/kg°C)\n",
    "lambda_v = 2.45 * 10**6  # Latent heat of vaporization (J/kg)\n",
    "gamma = 0.065  # Psychrometric constant (kPa/°C)\n",
    "\n",
    "# Function to calculate saturation vapor pressure (es) in kPa\n",
    "def saturation_vapor_pressure(T):\n",
    "    return 0.6108 * np.exp((17.27 * T) / (T + 237.3))\n",
    "\n",
    "# Function to calculate actual vapor pressure (ea) in kPa\n",
    "def actual_vapor_pressure(RH, T):\n",
    "    es = saturation_vapor_pressure(T)\n",
    "    return (RH / 100) * es\n",
    "\n",
    "# Function to calculate the slope of the saturation vapor pressure curve (Delta) in kPa/°C\n",
    "def delta_slope(T):\n",
    "    es = saturation_vapor_pressure(T)\n",
    "    return (4098 * es) / ((T + 237.3) ** 2)\n",
    "\n",
    "# Function to calculate Penman-Monteith Evaporation (mm/day)\n",
    "def penman_monteith_evaporation(Rn, T, RH):\n",
    "    # Assumed: G = 0 (soil heat flux density is negligible)\n",
    "    G = 0\n",
    "    \n",
    "    # Convert Radiation from W/m² to MJ/m²/10min\n",
    "    Rn_MJ_m2_10min = (Rn / 1000000) * 600\n",
    "    \n",
    "    # Calculate the saturation and actual vapor pressures\n",
    "    es = saturation_vapor_pressure(T)\n",
    "    ea = actual_vapor_pressure(RH, T)\n",
    "    \n",
    "    # Calculate the slope of the saturation vapor pressure curve\n",
    "    delta = delta_slope(T)\n",
    "    \n",
    "    # Calculate the Penman-Monteith evaporation (mm/10min)\n",
    "    numerator = delta * (Rn_MJ_m2_10min - G) + rho * cp * (es - ea) / lambda_v\n",
    "    denominator = delta + gamma * (1 + 0)  # Assuming r_s/r_a = 0 for simplicity\n",
    "    \n",
    "    E_t = numerator / denominator\n",
    "    return E_t\n",
    "\n",
    "# Replace '-' with NaN\n",
    "meteorological_data['Radiation (W/m2)'] = pd.to_numeric(meteorological_data['Radiation (W/m2)'], errors='coerce')\n",
    "meteorological_data['Temperature (°C)'] = pd.to_numeric(meteorological_data['Temperature (°C)'], errors='coerce')\n",
    "meteorological_data['Relative humidity (%)'] = pd.to_numeric(meteorological_data['Relative humidity (%)'], errors='coerce')\n",
    "\n",
    "\n",
    "# Replace negative values in specific columns\n",
    "columns_to_check = ['Radiation (W/m2)', 'Relative humidity (%)']\n",
    "meteorological_data[columns_to_check] = meteorological_data[columns_to_check].applymap(lambda x: np.nan if x < 0 else x)\n",
    "\n",
    "\n",
    "# Assuming your dataframe has the required columns: 'Radiation (W/m2)', 'Temperature (°C)', 'Relative humidity (%)'\n",
    "meteorological_data['Evaporation (mm/10min)'] = meteorological_data.apply(lambda row: penman_monteith_evaporation(\n",
    "    row['Radiation (W/m2)'], row['Temperature (°C)'], row['Relative humidity (%)']\n",
    "), axis=1)\n",
    "\n",
    "# Now df contains a new column 'Evaporation (mm/day)' with the calculated evaporation\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Station</th>\n",
       "      <th>Date &amp; Time (Winter)</th>\n",
       "      <th>Radiation (W/m2)</th>\n",
       "      <th>Relative humidity (%)</th>\n",
       "      <th>Temperature (°C)</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>hour</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>Evaporation (mm/10min)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>יוטבתה</td>\n",
       "      <td>01/01/2011 00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>יוטבתה</td>\n",
       "      <td>01/01/2011 00:10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>00:10:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>יוטבתה</td>\n",
       "      <td>01/01/2011 00:20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>00:20:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>יוטבתה</td>\n",
       "      <td>01/01/2011 00:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>00:30:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>יוטבתה</td>\n",
       "      <td>01/01/2011 00:40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>00:40:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Station Date & Time (Winter)  Radiation (W/m2)  \\\n",
       "0           0  יוטבתה     01/01/2011 00:00               NaN   \n",
       "1           1  יוטבתה     01/01/2011 00:10               NaN   \n",
       "2           2  יוטבתה     01/01/2011 00:20               NaN   \n",
       "3           3  יוטבתה     01/01/2011 00:30               NaN   \n",
       "4           4  יוטבתה     01/01/2011 00:40               NaN   \n",
       "\n",
       "   Relative humidity (%)  Temperature (°C)       Date      Time  hour  month  \\\n",
       "0                   64.0              14.2 2011-01-01  00:00:00     0      1   \n",
       "1                   68.0              13.3 2011-01-01  00:10:00     0      1   \n",
       "2                   69.0              13.1 2011-01-01  00:20:00     0      1   \n",
       "3                   71.0              12.5 2011-01-01  00:30:00     0      1   \n",
       "4                   73.0              12.2 2011-01-01  00:40:00     0      1   \n",
       "\n",
       "   year  Evaporation (mm/10min)  \n",
       "0  2011                     NaN  \n",
       "1  2011                     NaN  \n",
       "2  2011                     NaN  \n",
       "3  2011                     NaN  \n",
       "4  2011                     NaN  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteorological_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteorology.to_excel(path_files + \"\\\\Processed data\\\\Combined Meteorological data 2011 2023 radiation.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n"
     ]
    }
   ],
   "source": [
    "df_meteorology_periods = weather_data_Flowering_periods(meteorological_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteorology_periods.to_excel(path_files + \"\\\\Processed data\\\\Meteorological data 2011 2023 periods.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: yield_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_processing(processed_df):\n",
    "\n",
    "    yield_df = pd.read_excel(path_files + \"\\\\Processed data\\\\Yield 2015-2023.xlsx\", header=14)\n",
    "\n",
    "\n",
    "    for index, row in processed_df.iterrows():\n",
    "        yield_temp = yield_df.loc[yield_df['HAVA:PLOT'] == processed_df.at[index, \"Farm:Plot\"]]\n",
    "        if not yield_temp.empty:\n",
    "            if processed_df.at[index, 'Year'] != 2012:\n",
    "                yield_temp = yield_temp.fillna(0)\n",
    "                processed_df.at[index, 'Yield'] = yield_temp[f\"{processed_df.at[index, 'Year']} FINAL\"].values[0]\n",
    "        else:\n",
    "            processed_df.at[index, 'Yield'] = float(\"NaN\")\n",
    "        processed_df.loc[processed_df['Yield'] == 0, 'Yield'] = float(\"NaN\")\n",
    "\n",
    "    #Relative number of trees, then yield per tree\n",
    "    num_trees = pd.read_excel(path_files + \"\\\\Processed data\\\\Trees per plot.xlsx\")\n",
    "    num_trees['Age'] =  num_trees['גיל העץ']\n",
    "    num_trees.loc[(num_trees['Age'] < 4)  , 'percent_per_age_group'] = 0 #according to table recived by Tamarika\n",
    "    num_trees.loc[(num_trees['Age'] >= 4)& (num_trees['Age'] <= 6), 'percent_per_age_group'] = 20/150\n",
    "    num_trees.loc[(num_trees['Age'] >= 7)& (num_trees['Age'] <= 9), 'percent_per_age_group'] = 50/150\n",
    "    num_trees.loc[(num_trees['Age'] >= 10)& (num_trees['Age'] <= 12), 'percent_per_age_group'] = 80/150\n",
    "    num_trees.loc[(num_trees['Age'] >= 13)& (num_trees['Age'] <= 15), 'percent_per_age_group'] = 110/150\n",
    "    num_trees.loc[(num_trees['Age'] >= 16)& (num_trees['Age'] <= 19), 'percent_per_age_group'] = 136/150\n",
    "    num_trees.loc[(num_trees['Age'] >= 20), 'percent_per_age_group'] = 150/150\n",
    "    num_trees['Relative_number_of_adult_trees'] = num_trees['כמות עצים'] * num_trees['percent_per_age_group']\n",
    "\n",
    "    num_trees.rename(columns={'כמות עצים': 'Number_of_trees'}, inplace=True)\n",
    "    num_trees.rename(columns={'HAVA:PLOT':'Farm:Plot'}, inplace=True)\n",
    "\n",
    "    num_trees_grouped = num_trees.groupby(['Farm:Plot']).Number_of_trees.sum().reset_index()\n",
    "    num_trees_grouped.rename(columns={'Number_of_trees':'Number_of_trees_plot'}, inplace=True)\n",
    "    processed_df = pd.merge(processed_df, num_trees_grouped, on='Farm:Plot', how='inner')\n",
    "\n",
    "\n",
    "    num_trees_grouped = num_trees.groupby(['Farm:Plot']).Relative_number_of_adult_trees.sum().reset_index()\n",
    "    processed_df = pd.merge(processed_df, num_trees_grouped, on ='Farm:Plot', how='inner')\n",
    "\n",
    "    processed_df.loc[processed_df['Relative_number_of_adult_trees'] != 0, 'Relative_yield_per_adult_tree'] = processed_df['Yield'] / processed_df['Relative_number_of_adult_trees']\n",
    "    processed_df = processed_df.loc[:, ~processed_df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    # Relative contibution to yield for different sections of each plot (different ages of trees)\n",
    "\n",
    "    num_trees['Weight_age_group'] = num_trees['percent_per_age_group'] * 150\n",
    "\n",
    "    num_trees_processed = pd.DataFrame()\n",
    "\n",
    "    for plot in num_trees['Farm:Plot'].unique():\n",
    "        num_trees_temp = num_trees.loc[num_trees['Farm:Plot'] == plot]\n",
    "        num_trees_temp[\"total_weight\"] = num_trees_temp['Weight_age_group'] * num_trees_temp['Number_of_trees']\n",
    "        num_trees_temp['Relative_section_contribution_to_yield'] =  num_trees_temp[\"total_weight\"] /  num_trees_temp[\"total_weight\"].sum()\n",
    "        num_trees_temp = num_trees_temp[['Farm:Plot', 'Age', 'Weight_age_group', 'Number_of_trees', 'Relative_section_contribution_to_yield']]\n",
    "        num_trees_temp.rename(columns={'Number_of_trees':'Number_of_trees_section'}, inplace=True)\n",
    "        num_trees_processed = num_trees_processed.append(num_trees_temp, ignore_index=True)\n",
    "\n",
    "\n",
    "    processed_df = pd.merge(processed_df, num_trees_processed, on='Farm:Plot', how='inner')\n",
    "    processed_df['Relative_yield_from_section'] = processed_df['Yield'] * (processed_df['Relative_section_contribution_to_yield'])\n",
    "    processed_df['Relative_yield_per_tree'] = processed_df['Relative_yield_from_section'] / (processed_df['Number_of_trees_section'])\n",
    "    processed_df[['Number_of_trees_plot','Relative_number_of_adult_trees','Relative_yield_per_adult_tree', 'Number_of_trees_section', 'Relative_yield_per_tree']].replace(0, np.nan, inplace=True)\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Quality processing    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def quality_processing(processed_df):\n",
    "\n",
    "    processed_df['Farm'] = [x.partition(\":\")[0] for x in processed_df['Farm:Plot']]\n",
    "    quality_df = pd.read_csv(path_files + \"\\\\Raw data\\\\df_shilpuach_weight_quality_2012_2023.csv\")\n",
    "\n",
    "    quality_df['Farm'] = quality_df['Farm'].astype(str)\n",
    "    processed_df = pd.merge(processed_df, quality_df, on=[\"Farm\", \"Year\"], how=\"left\")\n",
    "\n",
    "    for column in quality_df.columns:\n",
    "        if column not in ['Farm', 'Year']:\n",
    "            new_column = f\"{column} from yield\"\n",
    "            processed_df[new_column] = processed_df[column] * processed_df['Relative_yield_per_tree']\n",
    "    return processed_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: create homogenic df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_homogemic_df(processed_data):\n",
    "    one_section_df = pd.DataFrame()\n",
    "    for plot in processed_data['Farm:Plot'].unique():\n",
    "        num_trees_temp = processed_data.loc[processed_data['Farm:Plot'] == plot]\n",
    "        for Date in num_trees_temp['Date'].unique():\n",
    "            temp = num_trees_temp.loc[num_trees_temp['Date'] == Date]\n",
    "            if temp.shape[0] == 1:\n",
    "                one_section_df = one_section_df.append(temp, ignore_index=True)\n",
    "    one_section_df.to_excel(path_files + \"\\\\Processed data\\\\Homogenic plots 2012-2023.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_temperature = ['t1', 't2', 't3', 't4', 't5', 't6', 't7', 't8', 't9', 't10', 't11', 't12']\n",
    "monthly_humidity = ['h1','h2','h3','h4','h5','h6','h7','h8','h9','h10', 'h11','h12']\n",
    "bloom_periods_temperature = ['T_Sorting', 'T_Flowering', 'T_Diluting', 'T_Growing', 'T_June_Dropping', 'T_Ripening', 'T_Harvest']\n",
    "bloom_periods_humidity = ['H_Sorting', 'H_Flowering', 'H_Diluting', 'H_Growing', 'H_June_Dropping', 'H_Ripening', 'H_Harvest']\n",
    "tree_age = ['Age']\n",
    "counts = ['AVG_clusters', 'AVG_fruitlets_upper', 'AVG_fruitlets_center', 'AVG_fruitlets_lower', 'SUM_tree_fruitlets', 'Days since january 1st']\n",
    "\n",
    "monthly_weather = []\n",
    "monthly_weather.extend(monthly_temperature)\n",
    "monthly_weather.extend(monthly_humidity)\n",
    "\n",
    "\n",
    "bloom_periods_weather = []\n",
    "bloom_periods_weather.extend(bloom_periods_temperature)\n",
    "bloom_periods_weather.extend(bloom_periods_humidity)\n",
    "\n",
    "all_features = []\n",
    "all_features.extend(bloom_periods_temperature)\n",
    "all_features.extend(bloom_periods_humidity)\n",
    "all_features.extend(counts)\n",
    "all_features_without_monthly = all_features.copy()\n",
    "all_features.extend(monthly_temperature)\n",
    "all_features.extend(monthly_humidity)\n",
    "\n",
    "quality_labels = ['0% to 5% from yield','25% to 40% from yield', '5% to 25% from yield','Other_shilpuah from yield', '15gr to 18gr from yield',\n",
    "                  '18gr to 23gr from yield', '23gr or more from yield', 'Other_weight from yield', 'small from yield']\n",
    "\n",
    "\n",
    "feature_dict = {'All features without monthly weather': all_features_without_monthly,\n",
    "                'Development periods weather': bloom_periods_weather,  'Monthly weather': monthly_weather,\n",
    "                'Counts data':counts, 'All features': all_features}\n",
    "\n",
    "\n",
    "feature_dict_with_age = {'All features without monthly weather': all_features_without_monthly, 'tree age':tree_age,\n",
    "                'Development periods weather': bloom_periods_weather,  'Monthly weather': monthly_weather,\n",
    "                'Counts data':counts, 'Counts and tree age':counts, 'All features': all_features}\n",
    "\n",
    "\n",
    "labels = ['Relative_yield_per_adult_tree', 'Relative_yield_per_tree']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def basic_model(data, feature_dict, labels):\n",
    "    df_results = pd.DataFrame()\n",
    "    for feature_set in feature_dict:\n",
    "        column_list = feature_dict[feature_set].copy()\n",
    "        column_list.extend(labels)\n",
    "        df = data[column_list].dropna()\n",
    "        print(\"\\nSet:\", feature_set)\n",
    "        X = df[feature_dict[feature_set]]\n",
    "        print(\"Number of samples:\", X.shape[0])\n",
    "        Y = df[labels]\n",
    "        for label in Y:\n",
    "            print(f\"\\n{label} using {feature_set}:\\n\")\n",
    "\n",
    "            for model in [xgb.XGBRegressor(verbosity=0), RandomForestRegressor(random_state=42), LinearRegression()]:\n",
    "                model_str = str(model)\n",
    "                if 'XGBRegressor' in model_str:\n",
    "                    model_str = 'XGBRegressor'\n",
    "\n",
    "                y = Y[label]\n",
    "\n",
    "                cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "                scores_r2 = cross_val_score(model, X, y, scoring='r2',\n",
    "                                            cv=cv, n_jobs=-1)\n",
    "\n",
    "                scores_rmse = cross_val_score(model, X, y,\n",
    "                                              scoring='neg_root_mean_squared_error',\n",
    "                                              cv=cv, n_jobs=-1)\n",
    "                scores_rmse = scores_rmse * -1\n",
    "\n",
    "                print(f\"{model_str} without reduction:\\nR2: {np.average(scores_r2)}, RMSE: { np.average(scores_rmse)}\")\n",
    "\n",
    "                df_results = df_results.append(\n",
    "                    {'Feature set': feature_set, 'Label': [label], 'Model': model_str,\n",
    "                     'r2': np.average(scores_r2),\n",
    "                     'RMSE': np.average(scores_rmse),\n",
    "                     'Num features': len(X.columns),\n",
    "                     'features': X.columns.values.tolist(),\n",
    "                     'feature_reduction': 'no', 'Data points': X.shape[0]}, ignore_index=True)\n",
    "\n",
    "                if feature_set != 'Baseline':\n",
    "                    fft = SequentialFeatureSelector(estimator=model, k_features='best', forward=True, scoring='r2',\n",
    "                                                    cv=cv)\n",
    "\n",
    "                    fft.fit(X, y)\n",
    "                    names_after_reduction = fft.k_feature_names_  # Chosen features after reduction\n",
    "                    names_after_reduction = list(names_after_reduction)\n",
    "\n",
    "                    scores_r2 = cross_val_score(model, X[names_after_reduction], y, scoring='r2',\n",
    "                                                cv=cv,\n",
    "                                                n_jobs=-1)  # Running the model on the chosen features from feature reduction\n",
    "                    scores_rmse = cross_val_score(model, X[names_after_reduction], y,\n",
    "                                                  scoring='neg_root_mean_squared_error',\n",
    "                                                  cv=cv, n_jobs=-1)\n",
    "                    scores_rmse = scores_rmse * -1\n",
    "\n",
    "                    print(f\"{model_str} with reduction:\\nR2: {np.average(scores_r2)}, RMSE: { np.average(scores_rmse)}\")\n",
    "\n",
    "                    df_results = df_results.append(\n",
    "                        {'Feature set': feature_set, 'Label': [label], 'Model': model_str,\n",
    "                         'r2': np.average(scores_r2),\n",
    "                         'RMSE': np.average(scores_rmse),\n",
    "                         'Num features': len(fft.k_feature_idx_),\n",
    "                         'features': names_after_reduction,\n",
    "                         'feature_reduction': 'yes', 'Data points': X.shape[0]}, ignore_index=True)\n",
    "                    # prediction = cross_val_predict(model, X[names_after_reduction], y, cv=cv)\n",
    "                    # prediction_df = pd.DataFrame()\n",
    "                    # prediction_df2 = pd.DataFrame(\n",
    "                    #     columns=['Label', 'Real_value', 'Prediction'])\n",
    "                    # prediction_df2['Real_value'] = y\n",
    "                    # prediction_df2['Prediction'] = prediction\n",
    "                    #\n",
    "                    # prediction_df2['Label'] = label\n",
    "                    #\n",
    "                    # prediction_df = pd.concat([prediction_df, prediction_df2])\n",
    "                    #\n",
    "                    # fig = plt.figure(figsize=(15, 10))\n",
    "                    # ax = sns.scatterplot(x=prediction_df['Real_value'], y=prediction_df['Prediction'], s=200)\n",
    "                    #\n",
    "                    # plt.xlabel(f'True value', fontsize=20)\n",
    "                    # plt.ylabel(f'Predicted value', fontsize=20)\n",
    "                    # lineStart = prediction_df.Real_value.min() - 0.1\n",
    "                    # lineEnd = prediction_df.Real_value.max() + 0.1\n",
    "                    # plt.plot([lineStart, lineEnd], [lineStart, lineEnd], 'k-', linestyle='dashed')\n",
    "                    # plt.suptitle(f\"True vs. Predicted tree yield\", fontsize=25)\n",
    "                    # plt.xlim(lineStart, lineEnd)\n",
    "                    # plt.ylim(lineStart, lineEnd)\n",
    "                    # plt.show()\n",
    "\n",
    "    return df_results\n",
    "\n",
    "def Feature_analysis(data, features, label):\n",
    "\n",
    "        column_list = features.copy()\n",
    "        column_list.append(label)\n",
    "        df = data[column_list].dropna()\n",
    "\n",
    "        X = df[features]\n",
    "        # corr tables\n",
    "        corrMatrix = df.corr()\n",
    "        mask = np.triu(np.ones_like(corrMatrix, dtype=bool))\n",
    "\n",
    "        ax = sns.heatmap(corrMatrix, annot_kws={\"size\": 10}, mask=mask, annot=True, vmin=-1, vmax=1)\n",
    "        # plt.suptitle(f'Correlations between {feature_set} and yield per tree', fontsize=35)\n",
    "        ax.set_xticklabels(list(corrMatrix.columns), rotation=30, fontsize=10)\n",
    "        ax.set_yticklabels(list(corrMatrix.columns), rotation=20, fontsize=10)\n",
    "        # plt.gca().invert_xaxis()\n",
    "        plt.show()\n",
    "\n",
    "        # SHAP\n",
    "        y = df[label]\n",
    "        shap.initjs()\n",
    "        xgb_shap = xgb.XGBRegressor(verbosity=0).fit(X, y)\n",
    "        explainer = shap.TreeExplainer(xgb_shap)\n",
    "        shap_values = explainer.shap_values(X)\n",
    "\n",
    "        shap.summary_plot(shap_values, X, show=False, plot_type=\"violin\")\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "\n",
    "        # plt.suptitle(f\"Feature importance for {label} prediction - {feature_set} set - 2024\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def age_model(data, feature_dict, label):\n",
    "    df_results = pd.DataFrame()\n",
    "    for feature_set in feature_dict:\n",
    "        feature_list = feature_dict[feature_set].copy()\n",
    "        if feature_set in ['All features without monthly weather', 'All features', 'Counts and tree age']:\n",
    "            feature_list.append('Age')\n",
    "        column_list = feature_list.copy()\n",
    "        column_list.extend(labels)\n",
    "        df = data[column_list].dropna()\n",
    "        print(\"\\nSet:\", feature_set)\n",
    "        X = df[feature_list]\n",
    "        print(\"Number of samples:\", X.shape[0])\n",
    "        Y = df[labels]\n",
    "        for label in Y:\n",
    "            print(f\"\\n{label} using {feature_set}:\\n\")\n",
    "            for model in [xgb.XGBRegressor(verbosity=0), RandomForestRegressor(random_state=42), LinearRegression()]:\n",
    "                model_str = str(model)\n",
    "                y = Y[label]\n",
    "                if 'XGBRegressor' in model_str:\n",
    "                    model_str = 'XGBRegressor'\n",
    "                cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "                scores_r2 = cross_val_score(model, X, y, scoring='r2',\n",
    "                                            cv=cv, n_jobs=-1)\n",
    "\n",
    "                scores_rmse = cross_val_score(model, X, y,\n",
    "                                              scoring='neg_root_mean_squared_error',\n",
    "                                              cv=cv, n_jobs=-1)\n",
    "                scores_rmse = scores_rmse * -1\n",
    "\n",
    "                print(f\"{model_str} without reduction:\\nR2: {np.average(scores_r2)}, RMSE: {np.average(scores_rmse)}\")\n",
    "\n",
    "                df_results = df_results.append(\n",
    "                    {'Feature set': feature_set, 'Label': [label], 'Model': model_str,\n",
    "                     'r2': np.average(scores_r2),\n",
    "                     'RMSE': np.average(scores_rmse),\n",
    "                     'Num features': len(X.columns),\n",
    "                     'features': X.columns.values.tolist(),\n",
    "                     'feature_reduction': 'no', 'Data points': X.shape[0]}, ignore_index=True)\n",
    "                if feature_set != 'Baseline':\n",
    "                    fft = SequentialFeatureSelector(estimator=model, k_features='best', forward=True, scoring='r2',\n",
    "                                                    cv=cv)\n",
    "\n",
    "                    fft.fit(X, y)\n",
    "                    names_after_reduction = fft.k_feature_names_  # Chosen features after reduction\n",
    "                    names_after_reduction = list(names_after_reduction)\n",
    "\n",
    "                    scores_r2 = cross_val_score(model, X[names_after_reduction], y, scoring='r2',\n",
    "                                                cv=cv,\n",
    "                                                n_jobs=-1)  # Running the model on the chosen features from feature reduction\n",
    "                    scores_rmse = cross_val_score(model, X[names_after_reduction], y,\n",
    "                                                  scoring='neg_root_mean_squared_error',\n",
    "                                                  cv=cv, n_jobs=-1)\n",
    "                    scores_rmse = scores_rmse * -1\n",
    "\n",
    "                    print(f\"{model_str} with reduction:\\nR2: {np.average(scores_r2)}, RMSE: {np.average(scores_rmse)}\")\n",
    "\n",
    "                    df_results = df_results.append(\n",
    "                        {'Feature set': feature_set, 'Label': [label], 'Model': model_str,\n",
    "                         'r2': np.average(scores_r2),\n",
    "                         'RMSE': np.average(scores_rmse),\n",
    "                         'Num features': len(fft.k_feature_idx_),\n",
    "                         'features': names_after_reduction,\n",
    "                         'feature_reduction': 'yes', 'Data points': X.shape[0]}, ignore_index=True)\n",
    "                    #\n",
    "                    # prediction = cross_val_predict(model, X[names_after_reduction], y, cv=cv)\n",
    "                    # prediction_df = pd.DataFrame()\n",
    "                    # prediction_df2 = pd.DataFrame(\n",
    "                    #     columns=['Label', 'Real_value', 'Prediction'])\n",
    "                    # prediction_df2['Real_value'] = y\n",
    "                    # prediction_df2['Prediction'] = prediction\n",
    "                    #\n",
    "                    # prediction_df2['Label'] = label\n",
    "                    #\n",
    "                    # prediction_df = pd.concat([prediction_df, prediction_df2])\n",
    "                    #\n",
    "                    #\n",
    "                    # fig = plt.figure(figsize=(15, 10))\n",
    "                    # ax = sns.scatterplot(x=prediction_df['Real_value'], y=prediction_df['Prediction'], s=200)\n",
    "                    #\n",
    "                    # plt.xlabel(f'True value', fontsize=20)\n",
    "                    # plt.ylabel(f'Predicted value', fontsize=20)\n",
    "                    # lineStart = prediction_df.Real_value.min() - 0.1\n",
    "                    # lineEnd = prediction_df.Real_value.max() + 0.1\n",
    "                    # plt.plot([lineStart, lineEnd], [lineStart, lineEnd], 'k-', linestyle='dashed')\n",
    "                    # plt.suptitle(f\"True vs. Predicted tree yield\", fontsize=25)\n",
    "                    # plt.xlim(lineStart, lineEnd)\n",
    "                    #\n",
    "                    # plt.ylim(lineStart, lineEnd)\n",
    "                    # plt.show()\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: lower_and_upper_error_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_and_upper_error_lists (df):\n",
    "  '''\n",
    "  this function returns a list of the upper and lower arror of each prediction. used for the error bar in the prediction scatter plot\n",
    "  '''\n",
    "  lower = []\n",
    "  upper = []\n",
    "  for i in range(len(df['Prediction'])):\n",
    "    true_label = df['Real_value'][i]\n",
    "    pred = df['Prediction'][i]\n",
    "    error = true_label - pred\n",
    "    if error > 0: #true label > pred : pred had a lower error\n",
    "      lower.append(error)\n",
    "      upper.append(0)\n",
    "    elif error < 0: #pred had an upper error: true<pred\n",
    "      lower.append(0)\n",
    "      upper.append(-error)\n",
    "    else: #pred was right on the money\n",
    "      lower.append(0)\n",
    "      upper.append(0)\n",
    "  lower = [x // 4 for x in lower]\n",
    "  upper =  [x // 4 for x in upper]\n",
    "  return lower, upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
